---
title: "HW 5"
author: "Jake Vettoretti"
date: "2025-11-06"
output: github_document
---

```{r}
library(broom)
library(tidyverse)
library(forcats)
set.seed(1)
```


## Problem 1


```{r}
# Define simulation function
bday_sim = function(n_room) {
  birthdays = sample(1:365, n_room, replace = TRUE)
  repeated_bday = length(unique(birthdays)) < n_room
  return(repeated_bday)
}

# Run simulation for group sizes 2‚Äì50, with 10,000 iterations each
bday_sim_results = 
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim) #returns logical vector
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat=mean(result)
  )

bday_sim_results

# Plot results
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    x = "Group size (n)",
    y = "Probability of shared birthday",
    title = "Probability that at least two people share a birthday"
  ) +
  theme_minimal()
```

Comments:
The plot shows that the probability that at least two people share a birthday increases quickly as the number of people in the room grows. When only a few people are present the chance of a shared birthday is very small but the probability rises sharply as the group becomes larger. Around twenty three people the probability reaches about fifty percent and by forty people it is close to one hundred percent. This finding illustrates the birthday paradox where even small groups have a surprisingly high chance of sharing a birthday because the number of possible pairs increases rapidly with group size.

## Question 2

Part 1, mu = 0

```{r}
#Setting the design elements
n = 30        # sample size
sigma = 5     # population SD
mu = 0        # true mean
n_sims = 5000 # number of simulated datasets

#Part 1, simulate 5000 datasets and test H0: mu=0
sim_mu0 =
  tibble(iter = 1:n_sims) |>
  mutate(
    results = map(iter, \(i) {
      tibble(x = rnorm(n, mean = mu, sd = sigma)) |>
        summarize(test = list(tidy(t.test(x, mu = 0)))) |>
        unnest(cols = c(test)) |>
        select(estimate, p.value)
    })
  ) |>
  unnest(cols = c(results))


#Part 1, checking model

sim_mu0 |>
  summarize(
    avg_mu_hat = mean(estimate),
    sd_mu_hat  = sd(estimate),
    type1error_rate = mean(p.value < 0.05)
  )
```

Part 2, mu = 1:6
```{r}
n = 30
sigma = 5
mu_values = 1:6
n_sims = 5000

# Repeat simulation for each Œº
sim_results =
  map_df(mu_values, \(m) {
    tibble(iter = 1:n_sims) |>
      mutate(
        results = map(iter, \(i) {
          tibble(x = rnorm(n, mean = m, sd = sigma)) |>
            summarize(test = list(tidy(t.test(x, mu = 0)))) |>
            unnest(cols = c(test)) |>
            select(estimate, p.value)
        })
      ) |>
      unnest(cols = c(results)) |>
      mutate(mu = m)
  })

# Check one example
sim_results |> 
  group_by(mu) |> 
  summarize(power = mean(p.value < 0.05))
```

Plotting power vs mu
```{r}
power_df =
  sim_results |>
  group_by(mu) |>
  summarize(
    power = mean(p.value < 0.05),
    .groups = "drop"
  )

power_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +
  labs(
    title = "Empirical Power vs True Mean (Œº)",
    x = "True Mean (Œº)",
    y = "Power (Proportion Rejecting H‚ÇÄ)"
  ) +
  theme_minimal(base_size = 14)
```

Describe the association between effect size and power:


Make a plot showing the average estimate of ùúáÃÇ on the y axis and the true value of ùúáon the x axis with overlay on the first the average estimate of ùúáÃÇ only in samples for which the null was rejected on the y axis and the true value of ùúáon the x axis.

```{r}
bias_df =
  sim_results |>
  group_by(mu) |>
  summarize(
    mean_est_all = mean(estimate),
    mean_est_sig = mean(estimate[p.value < 0.05]),
    .groups = "drop"
  )

bias_df

bias_df |>
  ggplot(aes(x = mu)) +
  geom_line(aes(y = mean_est_all, color = "All tests"), linewidth = 1) +
  geom_point(aes(y = mean_est_all, color = "All tests"), size = 2) +
  geom_line(aes(y = mean_est_sig, color = "Rejected H‚ÇÄ only"), linewidth = 1) +
  geom_point(aes(y = mean_est_sig, color = "Rejected H‚ÇÄ only"), size = 2) +
  labs(
    title = "Average Estimated Mean (ŒºÃÇ) vs True Mean (Œº)",
    x = "True Mean (Œº)",
    y = "Average Estimate (ŒºÃÇ)",
    color = ""
  ) +
  theme_minimal(base_size = 14)
```
 Is the sample average of ùúáÃÇ across tests for which the null is rejected approximately equal to the true value of ùúá? Why or why not?
 
 
 ## Problem 3
 
 Loading the data
```{r}
homicides <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```
Description of data:
This is a large dataset containing 52,179 observations and 12 different variables.
The dataset looks at the date of the victim's homicide, the victim's name, where the homicide occurred, and the case status of the homicide. It also includes demographics of the victims like age and sex.

Starting to clean and summarize
```{r}
#creating city_state
homicides = homicides |> 
  mutate(
    city_state = str_c(city, ", ", state)
  )

# summarise totals and unsolved by city
city_summary <- homicides |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE),
    .groups = "drop"
  ) |> 
  arrange(desc(total_homicides))

city_summary
```

Using prop.test function
```{r}
#filtering
baltimore_df =
  homicides |> 
  filter(city_state == "Baltimore, MD")

#obtaining counts for prop.test
n_unsolved = sum(baltimore_df$disposition %in% c("Closed without arrest", "Open/No arrest"))
n_total = nrow(baltimore_df)

#prop.test
baltimore_test = prop.test(x = n_unsolved, n = n_total)

#tidying
baltimore_tidy = tidy(baltimore_test)
baltimore_tidy

#pulling estimate and CI
baltimore_tidy |> 
  select(estimate, conf.low, conf.high)
```

Now running prop.test for each city in dataset
```{r}
city_results =
  city_summary |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, \(x, n) prop.test(x, n)),
    tidy_result = map(prop_test, tidy)
  ) |> 
  unnest(cols = tidy_result)

city_results |> 
  select(city_state, estimate, conf.low, conf.high)
```
Plotting the homicide estimates and CIs
```{r fig.width=14, fig.height=6}
#arranging cities by unsolved homicides
city_results |>
  ggplot(aes(x = fct_reorder(city_state, estimate), y = estimate)) +
  geom_point(color = "firebrick", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "gray40") +
  coord_flip() +   # ‚úÖ flips cities vertically, text now easy to read
  labs(
    title = "Proportion of Unsolved Homicides by City",
    subtitle = "With 95% Confidence Intervals (Washington Post Homicide Data)",
    x = NULL,
    y = "Estimated Proportion Unsolved"
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_minimal(base_size = 13) +
  theme(
    axis.text.y = element_text(size = 8),
    plot.title = element_text(face = "bold", size = 15),
    plot.subtitle = element_text(size = 11)
  )
```

 




